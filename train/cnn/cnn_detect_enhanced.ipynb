{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# List available devices\n",
    "devices = tf.config.list_physical_devices()\n",
    "print(\"Available devices:\")\n",
    "for device in devices:\n",
    "    print(device)\n",
    "\n",
    "# Check if TensorFlow is using the GPU\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, average_precision_score\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "DATASET_DIR = 'F:\\\\Study\\\\TU Dortmund\\\\Industrial Data Science 2\\\\Code\\\\Reference\\\\CNN\\\\train'\n",
    "CATEGORIES = ['met', 'nichtmet']\n",
    "IMG_SIZE = 128  # Selected image size\n",
    "\n",
    "# Function to load and preprocess the images\n",
    "def load_data():\n",
    "    data = []\n",
    "    for category in CATEGORIES:\n",
    "        path = os.path.join(DATASET_DIR, category)\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        for img in tqdm(os.listdir(path)):\n",
    "            try:\n",
    "                img_path = os.path.join(path, img)\n",
    "                img_array = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                img_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
    "                data.append([img_array, class_num])\n",
    "            except Exception as e:\n",
    "                pass\n",
    "    return data\n",
    "\n",
    "# Load and shuffle data\n",
    "data = load_data()\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Split data into features (X) and labels (y)\n",
    "X = np.array([item[0] for item in data]).reshape(-1, IMG_SIZE, IMG_SIZE, 1) / 255.0\n",
    "y = np.array([item[1] for item in data])\n",
    "\n",
    "# One-hot encode the labels\n",
    "y = to_categorical(y, num_classes=len(CATEGORIES))\n",
    "\n",
    "# Split data into training, validation, and testing sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Fit the data generator to the training data\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Build the CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    Dense(len(CATEGORIES), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(datagen.flow(X_train, y_train, batch_size=32),\n",
    "                    epochs=100,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Classification report\n",
    "report = classification_report(y_true_classes, y_pred_classes, target_names=CATEGORIES)\n",
    "print(report)\n",
    "\n",
    "# Precision and recall graph\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(len(CATEGORIES)):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(y_test[:, i], y_pred[:, i])\n",
    "    average_precision[i] = average_precision_score(y_test[:, i], y_pred[:, i])\n",
    "\n",
    "plt.figure()\n",
    "for i in range(len(CATEGORIES)):\n",
    "    plt.plot(recall[i], precision[i], lw=2, label=f'Precision-recall curve of class {CATEGORIES[i]} (AP={average_precision[i]:0.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation accuracy and loss\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "model.save('metal_classifier_model_optimized.keras')\n",
    "print(\"Model saved to metal_classifier_model_optimized.keras\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = load_model('metal_classifier_model_optimized.keras')\n",
    "\n",
    "# Preprocess image for classification\n",
    "def preprocess_image(image, img_size=128):\n",
    "    img_array = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    img_array = cv2.resize(img_array, (img_size, img_size))\n",
    "    img_array = img_array.reshape(-1, img_size, img_size, 1)\n",
    "    img_array = img_array / 255.0  # Normalize the image\n",
    "    return img_array\n",
    "\n",
    "# Classify the image as 'metal' or 'non-metal'\n",
    "def classify_image(image):\n",
    "    preprocessed_image = preprocess_image(image)\n",
    "    predictions = model.predict(preprocessed_image)\n",
    "    predicted_class = np.argmax(predictions, axis=1)[0]\n",
    "    categories = ['met', 'nichtmet']\n",
    "    confidence = predictions[0][predicted_class] * 100  # Confidence in percentage\n",
    "    return categories[predicted_class], confidence\n",
    "\n",
    "# Preprocess image for segmentation\n",
    "def preprocess_image_for_segmentation(image_path):\n",
    "    img_array = cv2.imread(image_path)\n",
    "    if img_array is None:\n",
    "        raise ValueError(f\"Image not found or unable to load: {image_path}\")\n",
    "    return img_array\n",
    "\n",
    "# Segment the image and draw bounding boxes\n",
    "def segment_and_draw_bounding_box(image_path):\n",
    "    img_array = preprocess_image_for_segmentation(image_path)\n",
    "    \n",
    "    _, binary_img = cv2.threshold(cv2.cvtColor(img_array, cv2.COLOR_BGR2GRAY), 127, 255, cv2.THRESH_BINARY_INV)\n",
    "    \n",
    "    contours, _ = cv2.findContours(binary_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Filter out smaller contours\n",
    "    contours = [contour for contour in contours if cv2.contourArea(contour) > 100]  # Adjust threshold as needed\n",
    "    \n",
    "    # Draw bounding boxes on the original image\n",
    "    img_color = img_array.copy()\n",
    "    for contour in contours:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        # Adjust bounding box coordinates based on contour dimensions\n",
    "        padding = 5  # Add some padding around the object\n",
    "        x1, y1 = max(0, x - padding), max(0, y - padding)\n",
    "        x2, y2 = min(img_array.shape[1], x + w + padding), min(img_array.shape[0], y + h + padding)\n",
    "        \n",
    "        # Crop the object from the original image\n",
    "        object_img = img_color[y1:y2, x1:x2]\n",
    "        predicted_category, confidence = classify_image(object_img)\n",
    "        \n",
    "        # Set bounding box color\n",
    "        color = (31, 119, 180) if predicted_category == 'nichtmet' else (255, 127, 14)\n",
    "        \n",
    "        # Draw the bounding box\n",
    "        cv2.rectangle(img_color, (x1, y1), (x2, y2), color, 2)  # Thicker bounding box for better visibility\n",
    "        \n",
    "        # Write category and confidence inside the bounding box\n",
    "        text = f'{predicted_category.capitalize()} ({confidence:.2f}%)'\n",
    "        font_scale = 0.5\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        (text_width, text_height), _ = cv2.getTextSize(text, font, font_scale, 1)\n",
    "        text_x = x1 + 5\n",
    "        text_y = y1 + text_height + 5\n",
    "        cv2.rectangle(img_color, (x1, y1), (x1 + text_width + 10, y1 + text_height + 10), color, cv2.FILLED)  # Background for text\n",
    "        cv2.putText(img_color, text, (text_x, text_y), font, font_scale, (0, 0, 0), 1)  # Black text\n",
    "    \n",
    "    # Display the image with bounding boxes and confidence\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(cv2.cvtColor(img_color, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Image path\n",
    "image_path = 'test/IMG20240609051857.jpg'\n",
    "\n",
    "# Try segmenting and drawing bounding boxes\n",
    "try:    \n",
    "    segment_and_draw_bounding_box(image_path)\n",
    "except ValueError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning on mobile data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model, Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "DATASET_DIR = 'F:\\\\Study\\\\TU Dortmund\\\\Industrial Data Science 2\\\\Code\\\\Reference\\\\CNN\\\\train'\n",
    "CATEGORIES = ['met', 'nichtmet']\n",
    "IMG_SIZE = 128\n",
    "MODEL_PATH = 'F:\\\\Study\\\\TU Dortmund\\\\Industrial Data Science 2\\\\Code\\\\Reference\\\\CNN\\\\metal_classifier_model_optimized.keras'\n",
    "NEW_MODEL_PATH = 'F:\\\\Study\\\\TU Dortmund\\\\Industrial Data Science 2\\\\Code\\\\Reference\\\\CNN\\\\metal_classifier_model_mobile.keras'\n",
    "RESULTS_DIR = 'results_' + datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Load and preprocess new dataset\n",
    "def load_data(dataset_dir, categories, img_size):\n",
    "    data = []\n",
    "    for category in categories:\n",
    "        path = os.path.join(dataset_dir, category)\n",
    "        class_num = categories.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_path = os.path.join(path, img)\n",
    "                img_array = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                img_array = cv2.resize(img_array, (img_size, img_size))\n",
    "                data.append([img_array, class_num])\n",
    "            except Exception as e:\n",
    "                pass\n",
    "    return data\n",
    "\n",
    "data = load_data(DATASET_DIR, CATEGORIES, IMG_SIZE)\n",
    "np.random.shuffle(data)\n",
    "\n",
    "X = np.array([item[0] for item in data]).reshape(-1, IMG_SIZE, IMG_SIZE, 1) / 255.0\n",
    "y = np.array([item[1] for item in data])\n",
    "\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=len(CATEGORIES))\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Data augmentation try 1\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Load pre-trained model\n",
    "base_model = load_model(MODEL_PATH)\n",
    "\n",
    "# Freeze some layers of the pre-trained model\n",
    "for layer in base_model.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add new top layers for fine-tuning\n",
    "model = Sequential(base_model.layers[:-1])\n",
    "model.add(Dense(len(CATEGORIES), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint(os.path.join(RESULTS_DIR, 'best_model.keras'), save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(datagen.flow(X_train, y_train, batch_size=32),\n",
    "                    epochs=500,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[early_stopping, checkpoint])\n",
    "\n",
    "# Load the best model\n",
    "model = load_model(os.path.join(RESULTS_DIR, 'best_model.keras'))\n",
    "\n",
    "# Save the final model\n",
    "model.save(NEW_MODEL_PATH)\n",
    "\n",
    "# Save training information\n",
    "def save_training_info(history, results_dir):\n",
    "    # Plot training & validation accuracy and loss\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.savefig(os.path.join(results_dir, 'accuracy_plot.png'))\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.savefig(os.path.join(results_dir, 'loss_plot.png'))\n",
    "\n",
    "    # Save history as text\n",
    "    with open(os.path.join(results_dir, 'training_history.txt'), 'w') as f:\n",
    "        f.write(str(history.history))\n",
    "\n",
    "save_training_info(history, RESULTS_DIR)\n",
    "\n",
    "print(f\"New model saved to {NEW_MODEL_PATH}\")\n",
    "print(f\"Training information saved to {RESULTS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define paths\n",
    "DATASET_DIR = 'F:\\\\Study\\\\TU Dortmund\\\\Industrial Data Science 2\\\\Code\\\\Reference\\\\CNN\\\\train'\n",
    "CATEGORIES = ['met', 'nichtmet']\n",
    "IMG_SIZE = 128\n",
    "MODEL_PATH = 'F:\\\\Study\\\\TU Dortmund\\\\Industrial Data Science 2\\\\Code\\\\Reference\\\\CNN\\\\metal_classifier_model_optimized.keras'\n",
    "NEW_MODEL_PATH = 'F:\\\\Study\\\\TU Dortmund\\\\Industrial Data Science 2\\\\Code\\\\Reference\\\\CNN\\\\metal_classifier_model_mobile.keras'\n",
    "RESULTS_DIR = 'results_' + datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Load and preprocess new dataset\n",
    "def load_data(dataset_dir, categories, img_size):\n",
    "    data = []\n",
    "    for category in categories:\n",
    "        path = os.path.join(dataset_dir, category)\n",
    "        class_num = categories.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_path = os.path.join(path, img)\n",
    "                img_array = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                img_array = cv2.resize(img_array, (img_size, img_size))\n",
    "                data.append([img_array, class_num])\n",
    "            except Exception as e:\n",
    "                pass\n",
    "    return data\n",
    "\n",
    "data = load_data(DATASET_DIR, CATEGORIES, IMG_SIZE)\n",
    "np.random.shuffle(data)\n",
    "\n",
    "X = np.array([item[0] for item in data]).reshape(-1, IMG_SIZE, IMG_SIZE, 1) / 255.0\n",
    "y = np.array([item[1] for item in data])\n",
    "\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=len(CATEGORIES))\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Load pre-trained model\n",
    "base_model = load_model(MODEL_PATH)\n",
    "\n",
    "# Freeze some layers of the pre-trained model\n",
    "for layer in base_model.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add new top layers for fine-tuning\n",
    "model = Sequential(base_model.layers[:-1])\n",
    "model.add(Dense(512, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(CATEGORIES), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint(os.path.join(RESULTS_DIR, 'best_model.keras'), save_best_only=True, monitor='val_loss', mode='min')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(datagen.flow(X_train, y_train, batch_size=16),\n",
    "                    epochs=100,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[early_stopping, checkpoint, reduce_lr])\n",
    "\n",
    "# Load the best model\n",
    "model = load_model(os.path.join(RESULTS_DIR, 'best_model.keras'))\n",
    "\n",
    "# Save the final model\n",
    "model.save(NEW_MODEL_PATH)\n",
    "\n",
    "# Save training information\n",
    "def save_training_info(history, results_dir):\n",
    "    # Plot training & validation accuracy and loss\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.savefig(os.path.join(results_dir, 'accuracy_plot.png'))\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.savefig(os.path.join(results_dir, 'loss_plot.png'))\n",
    "\n",
    "    # Save history as text\n",
    "    with open(os.path.join(results_dir, 'training_history.txt'), 'w') as f:\n",
    "        f.write(str(history.history))\n",
    "\n",
    "save_training_info(history, RESULTS_DIR)\n",
    "\n",
    "print(f\"New model saved to {NEW_MODEL_PATH}\")\n",
    "print(f\"Training information saved to {RESULTS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try 3\n",
    "\n",
    "Summary of Changes:\n",
    "Enhanced Data Augmentation: Increased rotation, shift, zoom, and added brightness augmentation.\n",
    "Fine-Tuned More Layers: Unfroze more layers for better feature extraction.\n",
    "Added Learning Rate Scheduler: Dynamically adjusted learning rate during training.\n",
    "Regularization Techniques: Increased dropout and L2 regularization to prevent overfitting.\n",
    "These adjustments should help you improve the model's performance and move closer to your target accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define paths\n",
    "DATASET_DIR = 'F:\\\\Study\\\\TU Dortmund\\\\Industrial Data Science 2\\\\Code\\\\Reference\\\\CNN\\\\train'\n",
    "CATEGORIES = ['met', 'nichtmet']\n",
    "IMG_SIZE = 128\n",
    "MODEL_PATH = 'F:\\\\Study\\\\TU Dortmund\\\\Industrial Data Science 2\\\\Code\\\\Reference\\\\CNN\\\\metal_classifier_model_optimized.keras'\n",
    "NEW_MODEL_PATH = 'F:\\\\Study\\\\TU Dortmund\\\\Industrial Data Science 2\\\\Code\\\\Reference\\\\CNN\\\\metal_classifier_model_mobile.keras'\n",
    "RESULTS_DIR = 'results_' + datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Load and preprocess new dataset\n",
    "def load_data(dataset_dir, categories, img_size):\n",
    "    data = []\n",
    "    for category in categories:\n",
    "        path = os.path.join(dataset_dir, category)\n",
    "        class_num = categories.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_path = os.path.join(path, img)\n",
    "                img_array = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                img_array = cv2.resize(img_array, (img_size, img_size))\n",
    "                data.append([img_array, class_num])\n",
    "            except Exception as e:\n",
    "                pass\n",
    "    return data\n",
    "\n",
    "data = load_data(DATASET_DIR, CATEGORIES, IMG_SIZE)\n",
    "np.random.shuffle(data)\n",
    "\n",
    "X = np.array([item[0] for item in data]).reshape(-1, IMG_SIZE, IMG_SIZE, 1) / 255.0\n",
    "y = np.array([item[1] for item in data])\n",
    "\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=len(CATEGORIES))\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    brightness_range=[0.2, 1.0],\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Load pre-trained model\n",
    "base_model = load_model(MODEL_PATH)\n",
    "\n",
    "# Freeze some layers of the pre-trained model\n",
    "for layer in base_model.layers[:-10]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add new top layers for fine-tuning\n",
    "model = Sequential(base_model.layers[:-1])\n",
    "model.add(Dense(512, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(CATEGORIES), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint(os.path.join(RESULTS_DIR, 'best_model.keras'), save_best_only=True, monitor='val_loss', mode='min')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "\n",
    "def lr_schedule(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return float(lr)\n",
    "    else:\n",
    "        return float(lr * tf.math.exp(-0.1))\n",
    "\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(datagen.flow(X_train, y_train, batch_size=16),\n",
    "                    epochs=100,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[early_stopping, checkpoint, reduce_lr, lr_scheduler])\n",
    "\n",
    "# Load the best model\n",
    "model = load_model(os.path.join(RESULTS_DIR, 'best_model.keras'))\n",
    "\n",
    "# Save the final model\n",
    "model.save(NEW_MODEL_PATH)\n",
    "\n",
    "# Save training information\n",
    "def save_training_info(history, results_dir):\n",
    "    # Plot training & validation accuracy and loss\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.savefig(os.path.join(results_dir, 'accuracy_plot.png'))\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.savefig(os.path.join(results_dir, 'loss_plot.png'))\n",
    "\n",
    "    # Save history as text\n",
    "    with open(os.path.join(results_dir, 'training_history.txt'), 'w') as f:\n",
    "        f.write(str(history.history))\n",
    "\n",
    "save_training_info(history, RESULTS_DIR)\n",
    "\n",
    "print(f\"New model saved to {NEW_MODEL_PATH}\")\n",
    "print(f\"Training information saved to {RESULTS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define paths\n",
    "DATASET_DIR = 'F:\\\\Study\\\\TU Dortmund\\\\Industrial Data Science 2\\\\Code\\\\Reference\\\\CNN\\\\train'\n",
    "CATEGORIES = ['met', 'nichtmet']\n",
    "IMG_SIZE = 128\n",
    "MODEL_PATH = 'F:\\\\Study\\\\TU Dortmund\\\\Industrial Data Science 2\\\\Code\\\\Reference\\\\CNN\\\\metal_classifier_model_optimized.keras'\n",
    "NEW_MODEL_PATH = 'F:\\\\Study\\\\TU Dortmund\\\\Industrial Data Science 2\\\\Code\\\\Reference\\\\CNN\\\\metal_classifier_model_mobile.keras'\n",
    "RESULTS_DIR = 'results_' + datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Load and preprocess new dataset\n",
    "def load_data(dataset_dir, categories, img_size):\n",
    "    data = []\n",
    "    for category in categories:\n",
    "        path = os.path.join(dataset_dir, category)\n",
    "        class_num = categories.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_path = os.path.join(path, img)\n",
    "                img_array = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                img_array = cv2.resize(img_array, (img_size, img_size))\n",
    "                data.append([img_array, class_num])\n",
    "            except Exception as e:\n",
    "                pass\n",
    "    return data\n",
    "\n",
    "data = load_data(DATASET_DIR, CATEGORIES, IMG_SIZE)\n",
    "np.random.shuffle(data)\n",
    "\n",
    "X = np.array([item[0] for item in data]).reshape(-1, IMG_SIZE, IMG_SIZE, 1) / 255.0\n",
    "y = np.array([item[1] for item in data])\n",
    "\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=len(CATEGORIES))\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=60,\n",
    "    width_shift_range=0.5,\n",
    "    height_shift_range=0.5,\n",
    "    shear_range=0.4,\n",
    "    zoom_range=0.5,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    brightness_range=[0.1, 1.2],\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Load pre-trained model\n",
    "base_model = load_model(MODEL_PATH)\n",
    "\n",
    "# Fine-tune more layers of the pre-trained model\n",
    "for layer in base_model.layers[:-10]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add new top layers for fine-tuning\n",
    "model = Sequential(base_model.layers[:-1])\n",
    "model.add(Dense(512, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(CATEGORIES), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint(os.path.join(RESULTS_DIR, 'best_model.keras'), save_best_only=True, monitor='val_loss', mode='min')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "\n",
    "def lr_schedule(epoch, lr):\n",
    "    return float(lr * tf.math.exp(-0.1))\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(datagen.flow(X_train, y_train, batch_size=16),\n",
    "                    epochs=100,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[early_stopping, checkpoint, reduce_lr, lr_scheduler])\n",
    "\n",
    "# Load the best model\n",
    "model = load_model(os.path.join(RESULTS_DIR, 'best_model.keras'))\n",
    "\n",
    "# Save the final model\n",
    "model.save(NEW_MODEL_PATH)\n",
    "\n",
    "# Save training information\n",
    "def save_training_info(history, results_dir):\n",
    "    # Plot training & validation accuracy and loss\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.savefig(os.path.join(results_dir, 'accuracy_plot.png'))\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.savefig(os.path.join(results_dir, 'loss_plot.png'))\n",
    "\n",
    "    # Save history as text\n",
    "    with open(os.path.join(results_dir, 'training_history.txt'), 'w') as f:\n",
    "        f.write(str(history.history))\n",
    "\n",
    "save_training_info(history, RESULTS_DIR)\n",
    "\n",
    "print(f\"New model saved to {NEW_MODEL_PATH}\")\n",
    "print(f\"Training information saved to {RESULTS_DIR}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
